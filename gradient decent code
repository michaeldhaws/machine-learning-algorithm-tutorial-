https://www.youtube.com/watch?v=uxFOst2mqAk

# gRADIENT DECENT FOR LINER AREGRESSION 
#YHAT = WX + B
# LOSS  = (Y-YHAT)**2 / N
import numpy as np
#Initialise some parameters
x = np.random.randn(10,1)
y = 2*x + np.random.rand()
# parameters
w =  0.0
b = 0.0

#hyperparameter 
learning_rate = 0.1

def descend(x, y, w, b , learning_rate):
    dldw = 0.0
    dldb = 0.0
    N = x #df is your dataframe
    N = x.shape[0]
    # loss = (y-(wx+b)))**2    
    for xi, yi in zip(x,y):
        dldw += -2*xi*(xi-(w*xi+b))
        dldb += -2*xi*(yi-(w*xi+b))
        
    w = w - learning_rate*(1/N)*dldw
    w = w - learning_rate*(1/N)*dldb
    
    return w,b
for epoch in range(400):
    w,b = descend(x,y,w,b,learning_rate)
    yhat = w*x + b
    loss = np.divide(np.sum((y-yhat)**2, axis=0), x.shape[0]) 
    print(f'{epoch} loss is {loss}, paramters w:{w}, b:{b}')
print(x,y)
